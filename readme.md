!python llama.cpp/convert_hf_to_gguf.py ./original_model/ --outtype bf16 --outfile ./quantized_model/bge-large-en-1.5.gguf 

#Exetue the above step after downloading model ! and create quantized_model !!!!

git clone https://github.com/abetlen/llama.cpp.git D:\\AI-models\\llama.cpp
